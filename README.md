# ğŸ§  Lokales Setup â€“ AI Assistant Projekt

Mit dieser Anleitung kannst du das Projekt lokal starten, inklusive Supabase & Ollama fÃ¼r RAG (Retrieval Augmented Generation).

---

## âœ… Voraussetzungen

Installiere bitte vorher:

- [Docker Desktop](https://www.docker.com/products/docker-desktop)
- [Node.js (empfohlen v18+)](https://nodejs.org)
- [Ollama](https://ollama.com)

---

## ğŸš€ Setup & Start (nur einmal ausfÃ¼hren)

```bash
# 1. AbhÃ¤ngigkeiten installieren
npm install

# 2. Supabase lokal starten (nur beim ersten Mal dauert das etwas)
npx supabase start

# 3. Docker-Setup starten (fÃ¼r Vector DB, Imgproxy etc.)
npm run docker:setup

# 4. Embedding-Modell fÃ¼r RAG laden
ollama pull mxbai-embed-large

# (Optional) LLM lokal laufen lassen, z.â€¯B. llama3
ollama run llama3.2:1b

# 5. Projekt starten
npm run dev
```
